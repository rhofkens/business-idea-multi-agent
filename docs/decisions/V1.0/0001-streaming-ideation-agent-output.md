# 1. Streaming Ideation Agent Output

*   **Status:** Proposed
*   **Date:** 2025-07-10

## Context and Problem Statement

The initial implementation of the Ideation Agent waits for the entire list of business ideas to be generated by the LLM before displaying any output to the user. For a list of 10 ideas, this can result in a significant delay (e.g., 30+ seconds) with no feedback, leading to a poor user experience. The user is left waiting without knowing if the process is working or stuck.

## Decision Drivers

*   **Improve User Experience:** Provide immediate, progressive feedback to the user.
*   **Responsiveness:** Make the application feel more interactive and alive.
*   **Resource Management:** Avoid holding the entire large response in memory before processing.

## Considered Options

1.  **Streaming Response:** Process the LLM response as a stream of text chunks. Parse complete JSON objects (business ideas) as they arrive and display them progressively.
2.  **Sequential Generation:** Request one idea at a time in a loop. This would provide incremental updates but would be much slower due to the overhead of multiple API calls.
3.  **Batch Generation:** Request ideas in smaller batches (e.g., 3 at a time). A middle-ground solution, but still involves periods of waiting.
4.  **UI-only Progress Indicators:** Keep the backend logic the same but add spinners or loading bars to the UI. This improves perceived performance but doesn't deliver results any faster.

## Decision Outcome

**Chosen Option:** Option 1, "Streaming Response".

This option was chosen because it directly addresses the root problem by delivering value (business ideas) to the user as soon as it becomes available. The `@openai/agents` SDK natively supports streaming via the `{ stream: true }` option in the `run` command, making this a well-supported and efficient solution.

The implementation will involve:
*   Updating the `ideationAgent` to return an `AsyncGenerator`.
*   Handling partial JSON data by buffering the incoming text stream.
*   Parsing and validating each business idea as it is fully received.
*   Updating the `agent-orchestrator` to iterate over the stream and display results in real-time.
*   Logging the raw text chunks to the console for immediate feedback and debugging, followed by the formatted display of each parsed idea.

## Consequences

*   **Positive:**
    *   Vastly improved user experience with real-time feedback.
    *   The application feels faster and more responsive.
    *   Reduced memory pressure as the full response is never held in memory at once.
*   **Negative:**
    *   Increased complexity in the agent's implementation due to the need to handle a stream and parse partial JSON.
    *   Error handling becomes more complex, as an error in the middle of the stream must be handled gracefully.
